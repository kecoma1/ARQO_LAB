Tengo 4 cores normales
Tengo 8 cores virtuales

EJ 0:
cat /proc/cpuinfo
    coger el cpu cores del cluster
    coger siblings del cluster
    
EJ 1:
    1.1. Yes it is possible. Yes it makes sense because you can be able to have multiple programs running at the same time.
    1.2.    ---- COMPROBAR NUMERO DE CORES EN EL ORDENA DE LA UNI
            ---- COMPROBAR NUMERO DE CORES EN EL cluster
            4 because my computer has 4 cores.
    1.3. The one with the highest priority is #pragma omp parallel num_threads(numthr), this is set before the creation of the threads. Then we have the function omp_set_num_threads(int num_threads), this function is called in the program before #pragma... the call of this function overwrites the value in OMP_NUM_THREADS. Finally, we have the last which is the ENVIROMENT variable, if we create a program which uses x threads and then we change the variable to y, the program wont change, once it's created it wont change (even when you change the value before the pragma creates the threades).
    1.4. It depends on which type of "privacy", if it is just private, each thread creates a copy of the variable but its initialized to 0, if you use firstprivate each thread has a copy of the variable but it also has the same value at the beggining.
    1.5. OMP creates a copy and initialise the variable to 0, if you used the normal private, but if you use the firstprivate the value is maintained
    1.6. As threads use copies of the variables the original variable remains the same.
    1.7. No, it depends on if the threads change the value.

EJ 2:
    2.1. The bigger the size, the bigger the time the function spends it. It has one loop, so the time to sum is linear, depends on the size.
    2.2. There's a problem with concurrence, the variable sum is read and written by all the threads, so the result is wrong.
    2.3 Yes it can be solved with both directives
        #pragma omp parallel for
        for(k=0;k<M;k++) {
            #pragma omp atomic
	    	sum = sum + A[k]*B[k];
    	}

        #pragma omp parallel for 
        for(k=0;k<M;k++) {
            #pragma omp critical 
            {
	    	    sum = sum + A[k]*B[k];
            }
        }
        As we are only executing one operation, making it atomic is our chosen option

    2.4. We will choose the reduction option, because it is the fastest, and it is the fastest because with the previous options we were preventing from concurrence problems the entire operation (+ and *) but we only need to protect from concurence the addition of the variable sum.
    2.5 EJECUTAR EN EL CLUSTER, CAMBIANDO EL NUMERO DE THREADS DEPENDIENDO DEL NUMERO DE CORES DEL NODO CLUSTER
    No, it is not always worth, for example, having a size of 40.000 and 2 threads makes the computation slower than computing without paralelism. 
    For sizes smaller than 40.000 having paralelism means that there is "too many people" working in the task. Open mp has to manage all the threads and that requires resources.
    No it is not, depending on the size of the task we may need a number x of threads to make the performance optimal, otherwise, the computation will be worse.
    ----Generar charts y responder ultima pregunta 2.5
    
EJ 4:
    4.1. 100000000 rectangles
    4.2. In pi_par4 there is a variable called "priv_sum", this is a local static variable created inside the pragma zone which is used in the loop and then the value is stored in the allocated memory. 
    In pi_par1 we use in the loop the variable which had allocated memory before creating the paralel zone, so managing concurrence makes the version 1 slower.
    Although we allocate memory for the number of threads, the variable is consider unique, therefore it cannot be accesed at the same time.
    4.3. Yes there are differences, the version 4 is faster than the version 1. This is due to the way the sumation is done. In the version 1 we use a common variable for all threads in each iteration of the loop, but in the version 4 we use a variable created inside the pragma zone, version only updates the value of that varaible when the loop is finished (once per thread). 
    4.4. The performance of the version 3 is faster. Version 3 checks the size of each block of the cache and then allocates memory for a varaible, so all the threads access at one block of the cache.
    4.5.
        1 0.395503
        2 0.343166
        3 0.307214
        4 0.324928
        5 0.327565
        6 0.349168
        7 0.242553
        8 0.222530
        9 0.271752
        10 0.192711
        12 0.283687
        It looks like the performance is better when the numbers are between 7-10 (both values included). 
    4.6. In the version 5 the variable used for the sumations is a copy of the one of the caller thread, in the version 4 each thread creates their unique static variable for the sumation and then update their part of the variable (with part we mean sum[0...]). 
    In the version 5 the critical zone is in the update part of the variable, this makes that this part cannot be done at the same time by different threads, but in the version 4 the variable is divided in the number of threads used, so each thread access to his part and updates the variable, without needing to check concurence. 
    4.7. The version 7 is the less efficient because, first it paralelizes the algorithm and creates some variables, after this it paralelizes the for loop so threads are able to work without concurence problems. In the version 6 with the "reduction" we paralelizes just the sumation of the loop, making the paralelization way more efficient.